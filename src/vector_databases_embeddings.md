# ğŸ“Œ Vector Databases & Embeddings
### ğŸ”¹ What are vectors?
- Vectors are arrays of numbers, usually in the range `-1 < n < 1`.  
- Example:  
  ```
  "ring" â†’ [-0.0097, 0.4465]
  ```
- These vectors are also called **embeddings** because they â€œembedâ€ the meaning of text (or images/audio) into a numeric space.

---

### ğŸ”¹ Creating embeddings
- Example sentence:  
  `"Who is the president of America in 2010?"`
- An embedding model (e.g., **gemini-embedding-001**, `text-embedding-3-small`) converts this text into a vector of 768â€“1536 dimensions.  
- Similar texts will produce **similar embeddings** (nearly same vectors).  
  ```
  "how are you" â†’ [0.123, -0.985, 0.452, ...]
  "how it is"   â†’ [0.124, -0.984, 0.451, ...]
  ```
  â†’ These two vectors are close to each other in vector space.

---

### ğŸ”¹ Why store vectors?
- To perform **semantic search** and **retrieval**.  
- Example:  
  - You save embeddings inside a **vector database** (e.g., Pinecone, Weaviate, MongoDB Atlas Vector).  
  - When a new query comes in, you embed it, then find the **nearest vectors** in the database.  
  - This allows finding semantically related documents, not just exact keyword matches.

---

# ğŸ“Œ NLP Pipeline Example

Sentence:  
**"The car is running on the highway at the speed 140km"**

1. **Tokenization**  
   - Breaks into meaningful tokens:  
     ```
     ["car", "run", "highway", "speed", "140km"]
     ```
   - Stopwords like *the, is, on, at* are ignored.

2. **Embedding**  
   - Each token converted to a vector:  
     ```
     "car"     â†’ [0.12, -0.55, 0.89, ...]
     "run"     â†’ [0.45,  0.21, -0.33, ...]
     "highway" â†’ [0.78, -0.14,  0.02, ...]
     ```

3. **Model (Gemini 2.0 Flash / GPT / etc.)**  
   - Uses embeddings + **transformer attention** to understand relationships:  
     - `"car"` â†” `"highway"`  
     - `"speed 140km"` â†” `"running"`

4. **Response Generation**  
   - Based on context, model generates a response:  
     ```
     "The car is moving very fast on the highway."
     ```

---

# ğŸ“Œ Cost & Tokens (STM vs LTM)

- Every word/token you send to an LLM = **cost**.  
- Example pricing (simplified):  
  - Input tokens (1M) â†’ $0.10  
  - Output tokens (1M) â†’ $0.40  

- If you keep sending the **entire conversation history (STM: Short-Term Memory)**, tokens pile up â†’ higher costs.  

ğŸ‘‰ Solution: **LTM (Long-Term Memory)**  
- Store older conversation embeddings in a **vector DB**.  
- Retrieve only the most relevant past chunks (e.g., last 7â€“20 messages).  
- Send only the useful context back to the LLM.  
- This reduces cost and keeps responses accurate.

---

âœ… So the full flow is:  
**Text â†’ Tokenization â†’ Embeddings â†’ Store in Vector DB (Pinecone/MongoDB) â†’ Retrieve relevant chunks â†’ Model reasoning â†’ Response**
